{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46e49a9",
   "metadata": {},
   "source": [
    "# PyTorch 數值型態與基本運算\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "041a2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "698408c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cpu'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bc118ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    \n",
    "    # Get the name of the current GPU\n",
    "    current_gpu_name = torch.cuda.get_device_name(0)  # Assuming you have at least one GPU\n",
    "    \n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    print(f\"Current GPU name: {current_gpu_name}\")\n",
    "       \n",
    "    # Now, operations will be performed on the GPU by default if available\n",
    "    cuda0 = torch.device('cuda:0')\n",
    "    tsr = torch.tensor([[1,2],[3,4],[5,6]], dtype=torch.float64, device=cuda0)\n",
    "else:\n",
    "    print(\"No GPU available. Using CPU.\")\n",
    "\n",
    "cuda0 = torch.device('cuda', 0)\n",
    "cuda1 = torch.device('cuda', 1)\n",
    "cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c27f8",
   "metadata": {},
   "source": [
    "## tensor 創建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b429c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsr = torch.tensor([[1,2],[3,4],[5,6]]); tsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63403d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "零矩陣:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "隨機矩陣:\n",
      "tensor([[0.1833, 0.4602, 0.2281],\n",
      "        [0.7871, 0.1627, 0.1672]])\n",
      "\n",
      "由數據構造的 Tensor:\n",
      "tensor([[1, 2, 5],\n",
      "        [3, 4, 7]])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 創建一個 2x3 的零矩陣\n",
    "tensor1 = torch.zeros(2, 3)\n",
    "print(\"零矩陣:\")\n",
    "print(tensor1)\n",
    "\n",
    "# 創建一個 2x3 的隨機矩陣\n",
    "tensor2 = torch.rand(2, 3)\n",
    "print(\"\\n隨機矩陣:\")\n",
    "print(tensor2)\n",
    "\n",
    "# 創建一個直接由數據構造的 Tensor\n",
    "data = [[1, 2, 5], [3, 4, 7]]\n",
    "tensor3 = torch.tensor(data)\n",
    "print(\"\\n由數據構造的 Tensor:\")\n",
    "print(tensor3)\n",
    "print(tensor3.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea27ed",
   "metadata": {},
   "source": [
    "## tensor data type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4acc7",
   "metadata": {},
   "source": [
    "\n",
    "![**tensor data type**](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*CHitOyDsG5fXhR80cAT3Ag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0730932b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsr = torch.tensor([[1,2],[3,4]], dtype=torch.float64); tsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "359e5367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor with a single element, 1, using the default tensor type (usually FloatTensor)\n",
    "tmp_tensor = torch.tensor([1])\n",
    "\n",
    "# Create a FloatTensor (32-bit floating point) with a single element, 1.01\n",
    "float_32 = torch.FloatTensor([1.01]) # 32 bits\n",
    "\n",
    "# Create a DoubleTensor (64-bit floating point) with a single element, 1.01\n",
    "float_64 = torch.DoubleTensor([1.01]) # 64 bits\n",
    "\n",
    "# Create an IntTensor (32-bit integer) with a single element, 1\n",
    "int_32 = torch.IntTensor([1]) # 32 bits\n",
    "\n",
    "# Create a LongTensor (64-bit integer) with a single element, 1\n",
    "int_64 = torch.LongTensor([1]) # 64 bits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "628513b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float_32 + float_32:\ttorch.float32\n",
      "float_32 * float_32:\ttorch.float32\n",
      "float_32 / float_32:\ttorch.float32\n",
      "------\n",
      "float_32 + float_64:\ttorch.float64\n",
      "float_32 * float_64:\ttorch.float64\n",
      "float_32 / float_64:\ttorch.float64\n",
      "------\n",
      "float_32 + int_32:\ttorch.float32\n",
      "float_32 * int_32:\ttorch.float32\n",
      "float_32 / int_32:\ttorch.float32\n",
      "------\n",
      "float_32 + int_64:\ttorch.float32\n",
      "float_32 * int_64:\ttorch.float32\n",
      "float_32 / int_64:\ttorch.float32\n",
      "------\n",
      "float_64 + float_32:\ttorch.float64\n",
      "float_64 * float_32:\ttorch.float64\n",
      "float_64 / float_32:\ttorch.float64\n",
      "------\n",
      "float_64 + float_64:\ttorch.float64\n",
      "float_64 * float_64:\ttorch.float64\n",
      "float_64 / float_64:\ttorch.float64\n",
      "------\n",
      "float_64 + int_32:\ttorch.float64\n",
      "float_64 * int_32:\ttorch.float64\n",
      "float_64 / int_32:\ttorch.float64\n",
      "------\n",
      "float_64 + int_64:\ttorch.float64\n",
      "float_64 * int_64:\ttorch.float64\n",
      "float_64 / int_64:\ttorch.float64\n",
      "------\n",
      "int_32 + float_32:\ttorch.float32\n",
      "int_32 * float_32:\ttorch.float32\n",
      "int_32 / float_32:\ttorch.float32\n",
      "------\n",
      "int_32 + float_64:\ttorch.float64\n",
      "int_32 * float_64:\ttorch.float64\n",
      "int_32 / float_64:\ttorch.float64\n",
      "------\n",
      "int_32 + int_32:\ttorch.int32\n",
      "int_32 * int_32:\ttorch.int32\n",
      "int_32 / int_32:\ttorch.int32\n",
      "------\n",
      "int_32 + int_64:\ttorch.int64\n",
      "int_32 * int_64:\ttorch.int64\n",
      "int_32 / int_64:\ttorch.int64\n",
      "------\n",
      "int_64 + float_32:\ttorch.float32\n",
      "int_64 * float_32:\ttorch.float32\n",
      "int_64 / float_32:\ttorch.float32\n",
      "------\n",
      "int_64 + float_64:\ttorch.float64\n",
      "int_64 * float_64:\ttorch.float64\n",
      "int_64 / float_64:\ttorch.float64\n",
      "------\n",
      "int_64 + int_32:\ttorch.int64\n",
      "int_64 * int_32:\ttorch.int64\n",
      "int_64 / int_32:\ttorch.int64\n",
      "------\n",
      "int_64 + int_64:\ttorch.int64\n",
      "int_64 * int_64:\ttorch.int64\n",
      "int_64 / int_64:\ttorch.int64\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "names = ['float_32', 'float_64', 'int_32', 'int_64']\n",
    "for i, tmp1 in enumerate([float_32, float_64, int_32, int_64]):\n",
    "    for j, tmp2 in enumerate([float_32, float_64, int_32, int_64]):\n",
    "        # print('{} + {}:\\t{}'.format(names[i], names[j], (tmp1+tmp2).dtype))\n",
    "        # print('{} * {}:\\t{}'.format(names[i], names[j], (tmp1*tmp2).dtype))\n",
    "        # print('{} / {}:\\t{}'.format(names[i], names[j], (tmp1/tmp2).dtype))\n",
    "\n",
    "\n",
    "        print(f\"{names[i]} + {names[j]}:\\t{(tmp1+tmp2).dtype}\")\n",
    "        print(f\"{names[i]} * {names[j]}:\\t{(tmp1+tmp2).dtype}\")\n",
    "        print(f\"{names[i]} / {names[j]}:\\t{(tmp1+tmp2).dtype}\")\n",
    "\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b85e18",
   "metadata": {},
   "source": [
    "### tensor <-----> numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0023d2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy [[1. 2.]\n",
      " [3. 4.]]\n",
      "type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# tensor transfer to numpy\n",
    "tsr2numpy = tsr.numpy()\n",
    "print('numpy', tsr2numpy)\n",
    "print('type:', type(tsr2numpy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a13aef4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor: tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "dtype: torch.int32\n",
      "tensor: tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "dtype: torch.float32\n",
      "tensor: tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "dtype: torch.int32\n",
      "tensor: tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "dtype: torch.int32\n"
     ]
    }
   ],
   "source": [
    "# numpy transfer to tensor\n",
    "\n",
    "'''\n",
    "torch.Tensor\n",
    "torch.tensor\n",
    "torch.as_tensor\n",
    "torch.from_numpy\n",
    "\n",
    "'''\n",
    "arr = np.array([[1,2,3],[4,5,6]])\n",
    "\n",
    "#1 sol\n",
    "arr2tsr_1 = torch.tensor(arr)\n",
    "print(\"tensor:\", arr2tsr_1)\n",
    "print(\"dtype:\", arr2tsr_1.dtype)\n",
    "\n",
    "#2 sol\n",
    "arr2tsr_2 = torch.Tensor(arr)\n",
    "print(\"tensor:\", arr2tsr_2)\n",
    "print(\"dtype:\", arr2tsr_2.dtype)\n",
    "\n",
    "#3 sol\n",
    "arr2tsr_3 = torch.as_tensor(arr)\n",
    "print(\"tensor:\", arr2tsr_3)\n",
    "print(\"dtype:\", arr2tsr_3.dtype)\n",
    "\n",
    "#4 sol\n",
    "arr2tsr_4 = torch.from_numpy(arr)\n",
    "print(\"tensor:\", arr2tsr_4)\n",
    "print(\"dtype:\", arr2tsr_4.dtype)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdbf0ac",
   "metadata": {},
   "source": [
    "## PyTorch Tensor 和資料型態\n",
    "\n",
    "#### 可微分Tensor的資料型態限制 :\n",
    "不可隨意改變資料型態：當Tensor被用作模型的參數，且是可微分的（參與反向傳播計算），其資料型態不能隨意改變。\n",
    "\n",
    "#### PyTorch模型參數的預設資料型態 :\n",
    "預設為Float32：PyTorch中模型參數的預設資料型態是32位浮點數（Float32）。這提供了足夠的精度，同時節省記憶體和計算時間。\n",
    "\n",
    "#### Numpy預設資料型態與轉換注意事項 :\n",
    "Numpy預設為Float64：Numpy的預設浮點數資料型態是Double（Float64），這與PyTorch的預設不同。\n",
    "轉換時的注意事項：在將Numpy陣列轉換成PyTorch Tensor時，需要特別注意這一點。直接轉換可能導致效能降低或不必要的資源消耗。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "986d7fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xdxd2\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\xdxd2\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型預測前 torch.float32\n",
      "模型預測前 torch.float32\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "mobilenet_v2 = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "mobilenet_v2.eval() \n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "print(\"模型預測前\", dummy_input.dtype)\n",
    "\n",
    "tmp = mobilenet_v2(dummy_input)\n",
    "\n",
    "print(\"模型預測前\", tmp.dtype)\n",
    "print('---------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c7fbb9",
   "metadata": {},
   "source": [
    "如果我們沒有注意到輸入的資料格式，直接把numpy的array轉成torch tensor輸入到模型內。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b93052e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- numpy array\n",
      "(224, 224, 3)\n",
      "float64\n",
      "\n",
      "- torch tensor\n",
      "torch.Size([3, 224, 224])\n",
      "torch.float64\n",
      "\n",
      "- expand dimension\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread('cat.jpg')\n",
    "img = cv2.resize(img, (224,224))\n",
    "\n",
    "# numpy array\n",
    "img = np.array(img)/255\n",
    "print(\"- numpy array\")\n",
    "print(img.shape)\n",
    "print(img.dtype)\n",
    "print(\"\")\n",
    "\n",
    "# torch array\n",
    "img = torch.tensor(img)\n",
    "\n",
    "'''\n",
    "permute: \n",
    "2 moves the original third dimension (color channels) to the first position.\n",
    "0 moves the original first dimension (height) to the second position.\n",
    "1 moves the original second dimension (width) to the third position.\n",
    "'''\n",
    "\n",
    "img = img.permute(2, 0, 1)\n",
    "print(\"- torch tensor\")\n",
    "print(img.shape)\n",
    "print(img.dtype)\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# change dimension\n",
    "img=img.unsqueeze(0)\n",
    "print(\"- expand dimension\")\n",
    "print(img.shape)\n",
    "print(img.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2e7cfda",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## float64 送進模型會報錯\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[43mmobilenet_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\mobilenetv2.py:174\u001b[0m, in \u001b[0;36mMobileNetV2.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\mobilenetv2.py:166\u001b[0m, in \u001b[0;36mMobileNetV2._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# This exists since TorchScript doesn't support inheritance, so the superclass method\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# (this one) needs to have a name other than `forward` that can be accessed in a subclass\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Cannot use \"squeeze\" as batch-size can be 1\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(x, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "## float64 送進模型會報錯\n",
    "\n",
    "tmp = mobilenet_v2(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ebfcf",
   "metadata": {},
   "source": [
    "避免這類型的出錯，盡量在numpy array或是image型態轉成torch tensor時候，就指定他是```torch.FloatTensor```。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53eab3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "img = cv2.imread('cat.jpg')\n",
    "img = cv2.resize(img, (224,224))\n",
    "img = np.array(img)/255\n",
    "\n",
    "\n",
    "# # 轉型 float32\n",
    "img = torch.FloatTensor(img)\n",
    "\n",
    "img = img.permute(2, 0, 1)\n",
    "\n",
    "img=img.unsqueeze(0)\n",
    "tmp = mobilenet_v2(img)\n",
    "print(tmp.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
