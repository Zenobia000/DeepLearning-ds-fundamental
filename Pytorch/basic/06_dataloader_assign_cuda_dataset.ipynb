{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color = white size=6 >如何將資料丟到CUDA。</font>**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確認 GPU 狀態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3090\n",
      "-----GPU can be used-----\n",
      "Using GPU 0: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# Get the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "if cuda_available:\n",
    "    # Print GPU information\n",
    "    for gpu_id in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(gpu_id)\n",
    "        print(f\"GPU {gpu_id}: {gpu_name}\")\n",
    "        \n",
    "else:\n",
    "    print(\"No CUDA-enabled GPU found.\")\n",
    "\n",
    "\n",
    "# Print the current GPU being used (if available)\n",
    "if cuda_available:\n",
    "    current_gpu = torch.cuda.current_device()\n",
    "    print(\"-----GPU can be used-----\")\n",
    "    print(f\"Using GPU {current_gpu}: {torch.cuda.get_device_name(current_gpu)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0+cu121'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Wed_Nov_22_10:30:42_Pacific_Standard_Time_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.107\n",
      "Build cuda_12.3.r12.3/compiler.33567101_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb  4 00:29:04 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.23                 Driver Version: 551.23         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   39C    P2            132W /  390W |    1303MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1344    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      2788    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A     10148    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     10960    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     11836    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A     12408    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     14640    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     18916    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A     21052    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     22064    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n",
      "|    0   N/A  N/A     25756    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     30368    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     32424      C   ...rograms\\Python\\Python311\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     37424    C+G   ...n\\120.0.2210.144\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     43228      C   ...rograms\\Python\\Python311\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     43764    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指定 GPU 運算\n",
    "\n",
    "在torch的tensor下直接.to(device)即可，但device需要先宣告。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/cv\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:15<00:00, 650385.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/cv\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../../data/cv\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data/cv\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 444021.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/cv\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../../data/cv\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data/cv\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:04<00:00, 388135.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/cv\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../../data/cv\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data/cv\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/cv\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../../data/cv\\MNIST\\raw\n",
      "\n",
      "tensor([5, 0])\n",
      "tensor([5, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "\n",
    "use_cuda = 1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() & use_cuda) else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "dataset_MNIST_tensor = datasets.MNIST('../../data/cv', train=True, download=True, transform=transform)\n",
    "mnistdata_loader = torch.utils.data.DataLoader(dataset_MNIST_tensor, batch_size=2)\n",
    "\n",
    "\n",
    "for data, target in mnistdata_loader:\n",
    "    print(target)\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    print(target)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU mode 的 tensor vs. CUDA mode 的 tensor  \n",
    "\n",
    "資料必須在相同硬體位置才可以做計算\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gup tensor add gpu tensor tensor([5., 7., 9.], device='cuda:0')\n",
      "cpu tensor add cpu tensor tensor([5., 7., 9.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu tensor add cpu tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,result_cpu)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 硬質硬體位置相加\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m result_cpu_gpu \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_cpu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_gpu\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu tensor add gpu tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,result_cpu_gpu)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 創建 CPU 張量\n",
    "tensor_cpu = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "\n",
    "# 創建 GPU 張量\n",
    "tensor_gpu = torch.tensor([4, 5, 6], dtype=torch.float32).cuda()\n",
    "\n",
    "\n",
    "# 將 CPU 張量搬到 GPU 上，進行相加\n",
    "result_gpu = tensor_cpu.to('cuda') + tensor_gpu\n",
    "print(\"gup tensor add gpu tensor\",result_gpu)\n",
    "\n",
    "# 將 GPU 張量搬到 CPU 上，然後相加\n",
    "result_cpu = tensor_cpu + tensor_gpu.to('cpu')\n",
    "print(\"cpu tensor add cpu tensor\",result_cpu)\n",
    "\n",
    "# 硬質硬體位置相加\n",
    "result_cpu_gpu = tensor_cpu + tensor_gpu\n",
    "print(\"cpu tensor add gpu tensor\",result_cpu_gpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 測試CPU和CUDA之間資料搬移"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When batch size = 10\n",
      "a forloop time for whole dataset within CPU: 2.253023862838745s\n",
      "a forloop time for whole dataset with CPU to CUDA: 4.19034218788147s\n",
      "\n",
      "When batch size = 2\n",
      "a forloop time for whole dataset within CPU: 2.874025583267212s\n",
      "a forloop time for whole dataset with CPU to CUDA: 8.376002311706543s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def test_time(data_loader):\n",
    "    start = time.time()\n",
    "    count=0\n",
    "    for data, target in data_loader:\n",
    "        count+=1\n",
    "    print(\"a forloop time for whole dataset within CPU: {}s\".format(time.time()-start))\n",
    "\n",
    "    start = time.time()\n",
    "    count=0\n",
    "    for data, target in data_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        count+=1\n",
    "    print(\"a forloop time for whole dataset with CPU to CUDA: {}s\".format(time.time()-start))\n",
    "    \n",
    "print(\"When batch size = 10\")\n",
    "mnistdata_loader = torch.utils.data.DataLoader(dataset_MNIST_tensor, batch_size=10, shuffle=False)\n",
    "test_time(mnistdata_loader)\n",
    "\n",
    "print(\"\\nWhen batch size = 2\")\n",
    "mnistdata_loader = torch.utils.data.DataLoader(dataset_MNIST_tensor, batch_size=2, shuffle=False)\n",
    "test_time(mnistdata_loader)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</font>**<font color = black size=4 >Note: </font>**<br>\n",
    "\n",
    "**<font color = black size=3 >1:資料跑來跑去一定花費data bandwidth，導致時間會變慢，所以在pytorch撰寫過程中要盡量避免資料在CPU和GPU之間跑來跑去。</font>**<br>\n",
    "\n",
    "**<font color = black size=3 >2: 容易造成CPU的tensor和CUDA的tensor進行運算的error，在進行運算要注意是在CPU還是CUDA。</font>**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
