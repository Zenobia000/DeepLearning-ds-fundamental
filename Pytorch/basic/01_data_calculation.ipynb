{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46e49a9",
   "metadata": {},
   "source": [
    "# 1-3 PyTorch 數值型態與基本運算\n",
    "\n",
    "##  PyTorch 數值型態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041a2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698408c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495f2ba",
   "metadata": {},
   "source": [
    "## 邏輯運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6f835c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7260,  0.4783, -0.2906,  0.4397,  0.9391],\n",
       "        [-2.0382, -1.1389,  0.8143, -0.0686, -0.3839],\n",
       "        [-0.3889, -1.8110,  0.5220, -0.7152, -1.1229],\n",
       "        [ 0.6399, -0.0161,  1.9484,  0.6155, -1.3006]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsr = torch.randn(4,5)\n",
    "tsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "791840d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9484)\n"
     ]
    }
   ],
   "source": [
    "# 1. max of entire tensor (torch.max(input) → Tensor)\n",
    "m = torch.max(tsr)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "618818ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6399, 0.4783, 1.9484, 0.6155, 0.9391])\n",
      "tensor([3, 0, 3, 3, 0])\n"
     ]
    }
   ],
   "source": [
    "# 2. max along a dimension (torch.max(input, dim, keepdim=False, *, out=None) → (Tensor, LongTensor))\n",
    "m, idx = torch.max(tsr,0)\n",
    "print(m)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d7f51f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6399, 0.4783, 1.9484, 0.6155, 0.9391])\n",
      "tensor([3, 0, 3, 3, 0])\n"
     ]
    }
   ],
   "source": [
    "# 2-2\n",
    "m, idx = torch.max(input=tsr,dim=0)\n",
    "print(m)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63579476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6399, 0.4783, 1.9484, 0.6155, 0.9391])\n",
      "tensor([3, 0, 3, 3, 0])\n",
      "(tensor([0.6399, 0.4783, 1.9484, 0.6155, 0.9391]), tensor([3, 0, 3, 3, 0]))\n"
     ]
    }
   ],
   "source": [
    "# 2-5\n",
    "p = (m,idx)\n",
    "torch.max(tsr,0,out=p)\n",
    "print(p[0])\n",
    "print(p[1])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4562e94b",
   "metadata": {},
   "source": [
    "## PyTorch基本運算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c457f077",
   "metadata": {},
   "source": [
    "### Numpy-Like Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcbbf273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "零矩陣:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "隨機矩陣:\n",
      "tensor([[0.1626, 0.2041, 0.9784],\n",
      "        [0.9071, 0.6297, 0.7966]])\n",
      "\n",
      "由數據構造的 Tensor:\n",
      "tensor([[1, 2, 5],\n",
      "        [3, 4, 7]])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 創建一個 2x3 的零矩陣\n",
    "tensor1 = torch.zeros(2, 3)\n",
    "print(\"零矩陣:\")\n",
    "print(tensor1)\n",
    "\n",
    "# 創建一個 2x3 的隨機矩陣\n",
    "tensor2 = torch.rand(2, 3)\n",
    "print(\"\\n隨機矩陣:\")\n",
    "print(tensor2)\n",
    "\n",
    "# 創建一個直接由數據構造的 Tensor\n",
    "data = [[1, 2, 5], [3, 4, 7]]\n",
    "tensor3 = torch.tensor(data)\n",
    "print(\"\\n由數據構造的 Tensor:\")\n",
    "print(tensor3)\n",
    "print(tensor3.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d225883b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor 加法結果:\n",
      "tensor([[1.1626, 2.2041, 5.9784],\n",
      "        [3.9071, 4.6297, 7.7966]])\n",
      "\n",
      "Tensor 乘法結果:\n",
      "tensor([[0.1626, 0.4083, 4.8919],\n",
      "        [2.7213, 2.5188, 5.5760]])\n",
      "\n",
      "重塑後的 Tensor:\n",
      "tensor([[1, 2, 5, 3, 4, 7]])\n"
     ]
    }
   ],
   "source": [
    "# Tensor 加法\n",
    "result_add = tensor2 + tensor3\n",
    "print(\"\\nTensor 加法結果:\")\n",
    "print(result_add)\n",
    "\n",
    "\n",
    "# Tensor 加法 (apple 2 apple)\n",
    "result_muiltiple = tensor2 * tensor3\n",
    "print(\"\\nTensor 乘法結果:\")\n",
    "print(result_muiltiple)\n",
    "\n",
    "\n",
    "# Tensor 改變形狀\n",
    "reshaped = tensor3.view(1, 6)\n",
    "print(\"\\n重塑後的 Tensor:\")\n",
    "print(reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dadb87",
   "metadata": {},
   "source": [
    "### 乘法比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baedb5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元素點對點相乘(方法1:np.multiply(a,b)):\n",
      "[[ 2  4  6]\n",
      " [12 15 18]]\n",
      "元素點對點相乘(方法2:a*b):\n",
      "[[ 2  4  6]\n",
      " [12 15 18]]\n",
      "\n",
      "矩陣相乘(方法1: np.dot(a,c)):\n",
      "[[22 28]\n",
      " [49 64]]\n",
      "矩陣相乘(方法2: a.dot(c)):\n",
      "[[22 28]\n",
      " [49 64]]\n",
      "矩陣相乘(方法3: np.matmul(a,c)):\n",
      "[[22 28]\n",
      " [49 64]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,2,3],\n",
    "              [4,5,6]])\n",
    "b = np.array([[2,2,2],\n",
    "              [3,3,3]])\n",
    "c = np.array([[1,2],\n",
    "              [3,4],\n",
    "              [5,6]])\n",
    "\n",
    "\n",
    "print(f'元素點對點相乘(方法1:np.multiply(a,b)):\\n{np.multiply(a,b)}')\n",
    "print(f'元素點對點相乘(方法2:a*b):\\n{a*b}')\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(f'矩陣相乘(方法1: np.dot(a,c)):\\n{np.dot(a,c)}')\n",
    "print(f'矩陣相乘(方法2: a.dot(c)):\\n{a.dot(c)}')\n",
    "print(f'矩陣相乘(方法3: np.matmul(a,c)):\\n{np.matmul(a,c)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62112eb",
   "metadata": {},
   "source": [
    "看起來```torch.matmul```和```torch.mm```都是進行矩陣內積運算，那```torch.matmul```和```torch.mm```有沒有什麼差異?\n",
    "\n",
    "```torch.mm```就是一般我們高中數學學的矩陣相乘，```torch.mm(a,b)```的情況，矩陣/向量$a$要能跟矩陣/向量$b$對上<br>\n",
    "$$a\\in R^{(m\\times n)}, b\\in R^{(n\\times k)}，torch.mm(a,b)\\in R^{(m\\times k)}$$\n",
    "\n",
    "```torch.matmul```: 除了一般的矩陣內積運算外，他可以達到Python內建廣播運算，叫做broadcasted運算，當兩個tensor的dimension是broadcasted，使用它時候會很有趣。<br>\n",
    "假設tensor a: $(i\\times 1\\times n\\times m)$ 和 tnesor b: $(k\\times m\\times p)$<br>\n",
    "torch.matmul(a,b) = $(i\\times k\\times n\\times p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7eb498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f48df7b0",
   "metadata": {},
   "source": [
    "## Torch broadcasted運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8dec2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: torch.Size([1, 2, 3])\n",
      "b: torch.Size([3, 1, 3, 2])\n",
      "a * b: torch.Size([3, 1, 2, 2])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[[2, 2, 2],\n",
    "                   [3, 3, 3]]])\n",
    "print(f'a: {a.shape}')\n",
    "\n",
    "\n",
    "b = torch.tensor([\n",
    "    [[[1, 1],\n",
    "      [1, 1],\n",
    "      [1, 1]]],\n",
    "    [[[2, 2],\n",
    "      [2, 2],\n",
    "      [2, 2]]],\n",
    "    [[[3, 3],\n",
    "      [3, 3],\n",
    "      [3, 3]]]\n",
    "])      \n",
    "print(f'b: {b.shape}')\n",
    "\n",
    "c = torch.matmul(a, b)\n",
    "print(f'a * b: {c.shape}')\n",
    "\n",
    "m = torch.zeros((3, 1, 2, 2))\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(b.shape[0]):\n",
    "        m[j, i, :, :] = torch.mm(a[i, :, :], b[j, 0, :, :])\n",
    "\n",
    "\n",
    "# 確認兩種算法是否一樣\n",
    "print((c - m).pow(2).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d6f95",
   "metadata": {},
   "source": [
    "## Numpy broadcasted運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1728ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: (1, 2, 3)\n",
      "b: (3, 1, 3, 2)\n",
      "a * b: (3, 1, 2, 2)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[[2, 2, 2],\n",
    "               [3, 3, 3]]])\n",
    "print(f'a: {a.shape}')\n",
    "\n",
    "b = np.array([\n",
    "    [[[1, 1],\n",
    "      [1, 1],\n",
    "      [1, 1]]],\n",
    "    [[[2, 2],\n",
    "      [2, 2],\n",
    "      [2, 2]]],\n",
    "    [[[3, 3],\n",
    "      [3, 3],\n",
    "      [3, 3]]]\n",
    "])      \n",
    "print(f'b: {b.shape}')\n",
    "\n",
    "c = np.matmul(a, b)\n",
    "print(f'a * b: {c.shape}')\n",
    "\n",
    "m = np.zeros((3, 1, 2, 2))\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(b.shape[0]):\n",
    "        m[j, i, :, :] = np.matmul(a[i, :, :], b[j, 0, :, :])\n",
    "\n",
    "\n",
    "# 確認兩種算法是否一樣\n",
    "print(((c - m)**2).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e1d69",
   "metadata": {},
   "source": [
    "## 效能比較 torch array vs numpy array\n",
    "### 利用broadcasted運算特性可以節省for loop的時間。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e90badc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: torch.Size([100, 5, 10])\n",
      "b: torch.Size([200, 1, 10, 20])\n",
      "a * b: torch.Size([200, 100, 5, 20])\n",
      "計算時間: 0.0660 seconds\n",
      "計算時間: 2.4513 seconds\n",
      "速度差異 37.14 倍\n",
      "差異總和: 0.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Define a function to format elapsed time\n",
    "def format_time(start_time):\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return f'{elapsed_time:.4f} seconds'  # Formats time to 4 decimal places\n",
    "\n",
    "# Create random tensors a and b\n",
    "a = torch.rand((100, 5, 10))\n",
    "b = torch.rand((200, 1, 10, 20))\n",
    "print(f'a: {a.shape}')\n",
    "print(f'b: {b.shape}')\n",
    "\n",
    "# Matrix multiplication using torch.matmul\n",
    "start_time = time.time()\n",
    "c = torch.matmul(a, b)\n",
    "print(f'a * b: {c.shape}')\n",
    "process_time_matmul = time.time() - start_time\n",
    "print(f'計算時間: {format_time(start_time)}')\n",
    "\n",
    "# Manual matrix multiplication using for loops\n",
    "start_time = time.time()\n",
    "m = torch.zeros((200, 100, 5, 20))\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(b.shape[0]):\n",
    "        m[j, i, :, :] = torch.mm(a[i, :, :], b[j, 0, :, :])\n",
    "process_time_loops = time.time() - start_time\n",
    "print(f'計算時間: {format_time(start_time)}')\n",
    "\n",
    "print(f\"速度差異 {process_time_loops/process_time_matmul:.2f} 倍\")\n",
    "# Calculate and print the absolute sum of differences\n",
    "difference = (c - m).abs().sum()\n",
    "print(f'差異總和: {difference}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc40dda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: (100, 5, 10)\n",
      "b: (200, 1, 10, 20)\n",
      "a * b: (200, 100, 5, 20)\n",
      "計算時間: 0.0311 seconds\n",
      "計算時間: 0.2624 seconds\n",
      "速度差異 8.44 倍\n",
      "差異總和: 0.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Define a function to format elapsed time\n",
    "def format_time(start_time):\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return f'{elapsed_time:.4f} seconds'  # Formats time to 4 decimal places\n",
    "\n",
    "# Create random tensors a and b\n",
    "a=np.random.rand(100,5,10)\n",
    "b=np.random.rand(200,1,10,20)\n",
    "print(f'a: {a.shape}')\n",
    "print(f'b: {b.shape}')\n",
    "\n",
    "# Matrix multiplication using torch.matmul\n",
    "start_time = time.time()\n",
    "c=np.matmul(a,b)\n",
    "print(f'a * b: {c.shape}')\n",
    "process_time_matmul = time.time() - start_time\n",
    "print(f'計算時間: {format_time(start_time)}')\n",
    "\n",
    "# Manual matrix multiplication using for loops\n",
    "start_time = time.time()\n",
    "m=np.zeros((200,100,5,20))\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(b.shape[0]):\n",
    "        m[j,i,:,:] = np.matmul(a[i,:,:], b[j,0,:,:])\n",
    "process_time_loops = time.time() - start_time\n",
    "print(f'計算時間: {format_time(start_time)}')\n",
    "\n",
    "print(f\"速度差異 {process_time_loops/process_time_matmul:.2f} 倍\")\n",
    "# Calculate and print the absolute sum of differences\n",
    "difference = np.abs((c - m)).sum()\n",
    "print(f'差異總和: {difference}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf23f39",
   "metadata": {},
   "source": [
    "### 線性代數函數庫 (Torch vs Numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae976b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy array\n",
      " [[6. 2.]\n",
      " [4. 5.]]\n",
      "torch array\n",
      " tensor([[6., 2.],\n",
      "        [4., 5.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "a_np = np.array([[6.0, 2.0],\n",
    "                 [4.0, 5.0]])\n",
    "a_torch = torch.tensor(a_np)\n",
    "\n",
    "\n",
    "print(\"numpy array\\n\", a_np)\n",
    "\n",
    "print(\"torch array\\n\", a_torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68ffdd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "tensor(9., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# normalization\n",
    "print(np.linalg.norm(a_np))\n",
    "print(torch.linalg.norm(a_torch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c06ad5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22727273 -0.09090909]\n",
      " [-0.18181818  0.27272727]]\n",
      "tensor([[ 0.2273, -0.0909],\n",
      "        [-0.1818,  0.2727]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "print(np.linalg.inv(a_np))\n",
    "print(torch.linalg.inv(a_torch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916798e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
