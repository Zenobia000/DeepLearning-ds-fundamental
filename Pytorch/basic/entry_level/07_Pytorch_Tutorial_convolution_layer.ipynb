{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.nn import Conv2d\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "import os\n",
        "import shutil\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "dataset = torchvision.datasets.CIFAR10(\"../data\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "dataloader = DataLoader(dataset, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## subdirectory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# subdirectory\n",
        "now = datetime.now()\n",
        "logdir = \"tb_logs/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
        "\n",
        "# Check if the directoryexists\n",
        "if os.path.exists(logdir):\n",
        "    # Delete the directory and all its contents\n",
        "    shutil.rmtree(logdir)\n",
        "\n",
        "# recreate the directory if plan to continue logging\n",
        "os.makedirs(logdir, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nn_model_convolution(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class nn_model_convolution(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(nn_model_convolution, self).__init__()\n",
        "        self.conv1 = Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        return x\n",
        "\n",
        "nn_model_convolution = nn_model_convolution()\n",
        "print(nn_model_convolution)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 3, 32, 32])\n",
            "torch.Size([64, 6, 30, 30])\n"
          ]
        }
      ],
      "source": [
        "writer = SummaryWriter(logdir)\n",
        "step = 0\n",
        "for data in dataloader:\n",
        "    \n",
        "    imgs, targets = data\n",
        "    output = nn_model_convolution(imgs)\n",
        "    print(imgs.shape)\n",
        "    print(output.shape)\n",
        "\n",
        "    #torch.size([64,3,32,32])\n",
        "    writer.add_images(\"input_convolution\", imgs, step)\n",
        "    #torch.size([64,6,30,30]) -> [xxx, 3, 30, 30]\n",
        "    \n",
        "    output = torch.reshape(output, (-1,3,30,30))\n",
        "    writer.add_images(\"output_convolution\", output, step)\n",
        "\n",
        "    step += 1\n",
        "    break\n",
        "\n",
        "writer.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import MaxPool2d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[2., 9., 6., 7., 4.],\n",
              "        [2., 9., 4., 2., 9.],\n",
              "        [8., 9., 5., 4., 4.],\n",
              "        [2., 7., 4., 7., 3.],\n",
              "        [4., 9., 2., 8., 2.]])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input = torch.randint(1,10,(5,5), dtype=torch.float32)\n",
        "input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 5, 5])\n"
          ]
        }
      ],
      "source": [
        "input = torch.reshape(input, (-1,1,5,5))\n",
        "print(input.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[9., 9.],\n",
            "          [9., 8.]]]])\n"
          ]
        }
      ],
      "source": [
        "class nn_model_pooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(nn_model_pooling, self).__init__()\n",
        "        self.maxpool1 = MaxPool2d(kernel_size=3, ceil_mode=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.maxpool1(input)\n",
        "        return output\n",
        "    \n",
        "nn_model_pooling = nn_model_pooling()\n",
        "output = nn_model_pooling(input)\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "writer = SummaryWriter(logdir)\n",
        "\n",
        "step = 0\n",
        "for data in dataloader:\n",
        "    imgs, targets = data\n",
        "    writer.add_images(\"input_pooling\", imgs, step)\n",
        "    output = nn_model_pooling(imgs)\n",
        "    writer.add_images(\"output_pooling\", output, step)\n",
        "    step += 1\n",
        "\n",
        "    break\n",
        "\n",
        "writer.close()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Pytorch Tutorial",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
