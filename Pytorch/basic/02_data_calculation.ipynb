{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46e49a9",
   "metadata": {},
   "source": [
    "# 1-3 PyTorch 數值型態與基本運算\n",
    "\n",
    "##  PyTorch 數值型態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041a2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "698408c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495f2ba",
   "metadata": {},
   "source": [
    "## 邏輯運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6f835c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5544,  0.3945,  0.7134, -0.9825,  2.3436],\n",
       "        [ 1.5518,  0.3083,  0.5637, -1.1906,  0.8133],\n",
       "        [ 0.6354,  1.1493,  0.2742,  0.6630,  0.4845],\n",
       "        [-1.5488, -0.0314,  1.7406,  0.3260, -0.6287]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsr = torch.randn(4,5)\n",
    "tsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "791840d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3436)\n"
     ]
    }
   ],
   "source": [
    "# 1. max of entire tensor (torch.max(input) → Tensor)\n",
    "m = torch.max(tsr)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "618818ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5518, 1.1493, 1.7406, 0.6630, 2.3436])\n",
      "tensor([1, 2, 3, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "# 2. max along a dimension (torch.max(input, dim, keepdim=False, *, out=None) → (Tensor, LongTensor))\n",
    "m, idx = torch.max(tsr,0)\n",
    "print(m)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d7f51f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5518, 1.1493, 1.7406, 0.6630, 2.3436])\n",
      "tensor([1, 2, 3, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "# 2-2\n",
    "m, idx = torch.max(input=tsr,dim=0)\n",
    "print(m)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63579476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5518, 1.1493, 1.7406, 0.6630, 2.3436])\n",
      "tensor([1, 2, 3, 2, 0])\n",
      "(tensor([1.5518, 1.1493, 1.7406, 0.6630, 2.3436]), tensor([1, 2, 3, 2, 0]))\n"
     ]
    }
   ],
   "source": [
    "# 2-5\n",
    "p = (m,idx)\n",
    "torch.max(tsr,0,out=p)\n",
    "print(p[0])\n",
    "print(p[1])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4562e94b",
   "metadata": {},
   "source": [
    "## PyTorch基本運算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c457f077",
   "metadata": {},
   "source": [
    "### Numpy-Like Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcbbf273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "零矩陣:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "隨機矩陣:\n",
      "tensor([[0.3339, 0.6048, 0.4548],\n",
      "        [0.2829, 0.2104, 0.6105]])\n",
      "\n",
      "由數據構造的 Tensor:\n",
      "tensor([[1, 2, 5],\n",
      "        [3, 4, 7]])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 創建一個 2x3 的零矩陣\n",
    "tensor1 = torch.zeros(2, 3)\n",
    "print(\"零矩陣:\")\n",
    "print(tensor1)\n",
    "\n",
    "# 創建一個 2x3 的隨機矩陣\n",
    "tensor2 = torch.rand(2, 3)\n",
    "print(\"\\n隨機矩陣:\")\n",
    "print(tensor2)\n",
    "\n",
    "# 創建一個直接由數據構造的 Tensor\n",
    "data = [[1, 2, 5], [3, 4, 7]]\n",
    "tensor3 = torch.tensor(data)\n",
    "print(\"\\n由數據構造的 Tensor:\")\n",
    "print(tensor3)\n",
    "print(tensor3.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d225883b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tensor 加法結果:\n",
      "tensor([[1.3339, 2.6048, 5.4548],\n",
      "        [3.2829, 4.2104, 7.6105]])\n",
      "\n",
      "Tensor 乘法結果:\n",
      "tensor([[0.3339, 1.2097, 2.2739],\n",
      "        [0.8486, 0.8415, 4.2738]])\n",
      "\n",
      "重塑後的 Tensor:\n",
      "tensor([[1, 2, 5, 3, 4, 7]])\n"
     ]
    }
   ],
   "source": [
    "# Tensor 加法\n",
    "result_add = tensor2 + tensor3\n",
    "print(\"\\nTensor 加法結果:\")\n",
    "print(result_add)\n",
    "\n",
    "\n",
    "# Tensor 加法 (apple 2 apple)\n",
    "result_muiltiple = tensor2 * tensor3\n",
    "print(\"\\nTensor 乘法結果:\")\n",
    "print(result_muiltiple)\n",
    "\n",
    "\n",
    "# Tensor 改變形狀\n",
    "reshaped = tensor3.view(1, 6)\n",
    "print(\"\\n重塑後的 Tensor:\")\n",
    "print(reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dadb87",
   "metadata": {},
   "source": [
    "### 乘法比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baedb5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元素點對點相乘(方法1:np.multiply(a,b)):\n",
      "[[ 2  4  6]\n",
      " [12 15 18]]\n",
      "元素點對點相乘(方法2:a*b):\n",
      "[[ 2  4  6]\n",
      " [12 15 18]]\n",
      "\n",
      "矩陣相乘(方法1: np.dot(a,c)):\n",
      "[[22 28]\n",
      " [49 64]]\n",
      "矩陣相乘(方法2: a.dot(c)):\n",
      "[[22 28]\n",
      " [49 64]]\n",
      "矩陣相乘(方法3: np.matmul(a,c)):\n",
      "[[22 28]\n",
      " [49 64]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,2,3],\n",
    "              [4,5,6]])\n",
    "b = np.array([[2,2,2],\n",
    "              [3,3,3]])\n",
    "c = np.array([[1,2],\n",
    "              [3,4],\n",
    "              [5,6]])\n",
    "\n",
    "\n",
    "print(f'元素點對點相乘(方法1:np.multiply(a,b)):\\n{np.multiply(a,b)}')\n",
    "print(f'元素點對點相乘(方法2:a*b):\\n{a*b}')\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(f'矩陣相乘(方法1: np.dot(a,c)):\\n{np.dot(a,c)}')\n",
    "print(f'矩陣相乘(方法2: a.dot(c)):\\n{a.dot(c)}')\n",
    "print(f'矩陣相乘(方法3: np.matmul(a,c)):\\n{np.matmul(a,c)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62112eb",
   "metadata": {},
   "source": [
    "# torch.matmul 與 torch.mm 的差異\n",
    "\n",
    "在 PyTorch 中，`torch.matmul` 和 `torch.mm` 都用於執行矩陣內積運算，但它們在功能和應用場景上存在一定的差異。\n",
    "\n",
    "## torch.mm\n",
    "\n",
    "- `torch.mm` 執行的是標準的矩陣乘法運算。\n",
    "- 用法: `torch.mm(a, b)`，其中 `a` 和 `b` 是矩陣。\n",
    "- 約束: 矩陣 `a` 的列數必須與矩陣 `b` 的行數相同。\n",
    "- 維度要求: 如果矩陣 `a` 是 \\(m \\times n\\)，矩陣 `b` 是 \\(n \\times k\\)，則結果矩陣將是 \\(m \\times k\\)。\n",
    "- 例子: 若 `a` ∈ \\(R^{m \\times n}\\) 且 `b` ∈ \\(R^{n \\times k}\\)，則 `torch.mm(a, b)` ∈ \\(R^{m \\times k}\\)。\n",
    "\n",
    "## torch.matmul\n",
    "\n",
    "- `torch.matmul` 提供了更廣泛的矩陣乘法運算，包括標準矩陣乘法、點乘以及維度廣播。\n",
    "- 廣播運算: `torch.matmul` 支持廣播運算，能夠自動擴展維度以適應矩陣運算的需求。\n",
    "- 維度要求: 對於高維張量，`torch.matmul` 可以處理形如 \\(i \\times 1 \\times n \\times m\\) 和 \\(k \\times m \\times p\\) 的張量，並輸出形為 \\(i \\times k \\times n \\times p\\) 的張量。\n",
    "- 靈活性: `torch.matmul` 在處理不同維度張量時提供了更大的靈活性和應用範圍。\n",
    "\n",
    "## 總結\n",
    "\n",
    "雖然 `torch.mm` 和 `torch.matmul` 都可用於矩陣乘法，但 `torch.matmul` 提供了更廣泛的功能，特別是在處理高維數據和需要廣播功能的場景中。選擇使用哪一個函數，應根據具體的應用需求和矩陣的維度來決定。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48df7b0",
   "metadata": {},
   "source": [
    "## Torch broadcasted運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8dec2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: torch.Size([1, 2, 3])\n",
      "b: torch.Size([3, 1, 3, 2])\n",
      "a * b: torch.Size([3, 1, 2, 2])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[[2, 2, 2],\n",
    "                   [3, 3, 3]]])\n",
    "print(f'a: {a.shape}')\n",
    "\n",
    "\n",
    "b = torch.tensor([\n",
    "    [[[1, 1],\n",
    "      [1, 1],\n",
    "      [1, 1]]],\n",
    "    [[[2, 2],\n",
    "      [2, 2],\n",
    "      [2, 2]]],\n",
    "    [[[3, 3],\n",
    "      [3, 3],\n",
    "      [3, 3]]]\n",
    "])      \n",
    "print(f'b: {b.shape}')\n",
    "\n",
    "c = torch.matmul(a, b)\n",
    "print(f'a * b: {c.shape}')\n",
    "\n",
    "m = torch.zeros((3, 1, 2, 2))\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(b.shape[0]):\n",
    "        m[j, i, :, :] = torch.mm(a[i, :, :], b[j, 0, :, :])\n",
    "\n",
    "\n",
    "# 確認兩種算法是否一樣\n",
    "print((c - m).pow(2).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d6f95",
   "metadata": {},
   "source": [
    "## Numpy broadcasted運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1728ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: (1, 2, 3)\n",
      "b: (3, 1, 3, 2)\n",
      "a * b: (3, 1, 2, 2)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[[2, 2, 2],\n",
    "               [3, 3, 3]]])\n",
    "print(f'a: {a.shape}')\n",
    "\n",
    "b = np.array([\n",
    "    [[[1, 1],\n",
    "      [1, 1],\n",
    "      [1, 1]]],\n",
    "    [[[2, 2],\n",
    "      [2, 2],\n",
    "      [2, 2]]],\n",
    "    [[[3, 3],\n",
    "      [3, 3],\n",
    "      [3, 3]]]\n",
    "])      \n",
    "print(f'b: {b.shape}')\n",
    "\n",
    "c = np.matmul(a, b)\n",
    "print(f'a * b: {c.shape}')\n",
    "\n",
    "m = np.zeros((3, 1, 2, 2))\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(b.shape[0]):\n",
    "        m[j, i, :, :] = np.matmul(a[i, :, :], b[j, 0, :, :])\n",
    "\n",
    "\n",
    "# 確認兩種算法是否一樣\n",
    "print(((c - m)**2).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e1d69",
   "metadata": {},
   "source": [
    "## 效能比較 torch array vs numpy array\n",
    "### 利用broadcasted運算特性可以節省for loop的時間。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e90badc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: torch.Size([100, 5, 10])\n",
      "b: torch.Size([200, 1, 10, 20])\n",
      "a * b: torch.Size([200, 100, 5, 20])\n",
      "計算時間: 0.0437 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "計算時間: 1.2920 seconds\n",
      "速度差異 29.60 倍\n",
      "差異總和: 0.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Define a function to format elapsed time\n",
    "def format_time(start_time):\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return f'{elapsed_time:.4f} seconds'  # Formats time to 4 decimal places\n",
    "\n",
    "# Create random tensors a and b\n",
    "a = torch.rand((100, 5, 10))\n",
    "b = torch.rand((200, 1, 10, 20))\n",
    "print(f'a: {a.shape}')\n",
    "print(f'b: {b.shape}')\n",
    "\n",
    "# Matrix multiplication using torch.matmul\n",
    "start_time = time.time()\n",
    "c = torch.matmul(a, b)\n",
    "print(f'a * b: {c.shape}')\n",
    "process_time_matmul = time.time() - start_time\n",
    "print(f'計算時間: {format_time(start_time)}')\n",
    "\n",
    "# Manual matrix multiplication using for loops\n",
    "start_time = time.time()\n",
    "m = torch.zeros((200, 100, 5, 20))\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(b.shape[0]):\n",
    "        m[j, i, :, :] = torch.mm(a[i, :, :], b[j, 0, :, :])\n",
    "process_time_loops = time.time() - start_time\n",
    "print(f'計算時間: {format_time(start_time)}')\n",
    "\n",
    "print(f\"速度差異 {process_time_loops/process_time_matmul:.2f} 倍\")\n",
    "# Calculate and print the absolute sum of differences\n",
    "difference = (c - m).abs().sum()\n",
    "print(f'差異總和: {difference}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc40dda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: (100, 5, 10)\n",
      "b: (200, 1, 10, 20)\n",
      "a * b: (200, 100, 5, 20)\n",
      "計算時間: 0.0161 seconds\n",
      "計算時間: 0.1454 seconds\n",
      "速度差異 9.00 倍\n",
      "差異總和: 0.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# Define a function to format elapsed time\n",
    "def format_time(start_time):\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return f'{elapsed_time:.4f} seconds'  # Formats time to 4 decimal places\n",
    "\n",
    "# Create random tensors a and b\n",
    "a=np.random.rand(100,5,10)\n",
    "b=np.random.rand(200,1,10,20)\n",
    "print(f'a: {a.shape}')\n",
    "print(f'b: {b.shape}')\n",
    "\n",
    "# Matrix multiplication using torch.matmul\n",
    "start_time = time.time()\n",
    "c=np.matmul(a,b)\n",
    "print(f'a * b: {c.shape}')\n",
    "process_time_matmul = time.time() - start_time\n",
    "print(f'計算時間: {format_time(start_time)}')\n",
    "\n",
    "# Manual matrix multiplication using for loops\n",
    "start_time = time.time()\n",
    "m=np.zeros((200,100,5,20))\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(b.shape[0]):\n",
    "        m[j,i,:,:] = np.matmul(a[i,:,:], b[j,0,:,:])\n",
    "process_time_loops = time.time() - start_time\n",
    "print(f'計算時間: {format_time(start_time)}')\n",
    "\n",
    "print(f\"速度差異 {process_time_loops/process_time_matmul:.2f} 倍\")\n",
    "# Calculate and print the absolute sum of differences\n",
    "difference = np.abs((c - m)).sum()\n",
    "print(f'差異總和: {difference}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d6e5a",
   "metadata": {},
   "source": [
    "# 線性代數運算操作比較 (Torch vs Numpy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf23f39",
   "metadata": {},
   "source": [
    "### 常見線性代數操作比較 (Torch vs Numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "544bf161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 2.]\n",
      " [4. 5.]]\n",
      "tensor([[6., 2.],\n",
      "        [4., 5.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 給定的 NumPy 陣列和 PyTorch 張量\n",
    "a_np = np.array([[6.0, 2.0],\n",
    "                 [4.0, 5.0]])\n",
    "a_torch = torch.tensor(a_np, dtype=torch.float)\n",
    "\n",
    "print(a_np)\n",
    "\n",
    "print(a_torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32cfbbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Transposed:\n",
      " [[6. 4.]\n",
      " [2. 5.]]\n",
      "PyTorch Transposed:\n",
      " tensor([[6., 4.],\n",
      "        [2., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# 1. 矩陣轉置\n",
    "a_np_transposed = a_np.T\n",
    "a_torch_transposed = a_torch.t()\n",
    "\n",
    "print(\"NumPy Transposed:\\n\", a_np_transposed)\n",
    "print(\"PyTorch Transposed:\\n\", a_torch_transposed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7584ddaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Matrix Multiplication:\n",
      " [[40. 34.]\n",
      " [34. 41.]]\n",
      "PyTorch Matrix Multiplication:\n",
      " tensor([[40., 34.],\n",
      "        [34., 41.]])\n"
     ]
    }
   ],
   "source": [
    "# 2. 矩陣乘法 (以自身為例)\n",
    "a_np_mult = np.dot(a_np, a_np_transposed)\n",
    "a_torch_mult = torch.mm(a_torch, a_torch_transposed)\n",
    "\n",
    "print(\"NumPy Matrix Multiplication:\\n\", a_np_mult)\n",
    "print(\"PyTorch Matrix Multiplication:\\n\", a_torch_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ec28ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Inverse:\n",
      " [[ 0.22727273 -0.09090909]\n",
      " [-0.18181818  0.27272727]]\n",
      "PyTorch Inverse:\n",
      " tensor([[ 0.2273, -0.0909],\n",
      "        [-0.1818,  0.2727]])\n"
     ]
    }
   ],
   "source": [
    "# 3. 求逆矩陣 (注意: 僅當矩陣可逆時有效)\n",
    "a_np_inverse = np.linalg.inv(a_np)\n",
    "a_torch_inverse = torch.inverse(a_torch)\n",
    "\n",
    "print(\"NumPy Inverse:\\n\", a_np_inverse)\n",
    "print(\"PyTorch Inverse:\\n\", a_torch_inverse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37b581da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Determinant: 22.000000000000004\n",
      "PyTorch Determinant: tensor(22.)\n"
     ]
    }
   ],
   "source": [
    "# 4. 行列式計算\n",
    "a_np_det = np.linalg.det(a_np)\n",
    "a_torch_det = torch.det(a_torch)\n",
    "\n",
    "print(\"NumPy Determinant:\", a_np_det)\n",
    "print(\"PyTorch Determinant:\", a_torch_det)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756784c5",
   "metadata": {},
   "source": [
    "## 向量範數計算類型\n",
    "\n",
    "在線性代數中，「範數」是用於衡量向量大小的一種重要概念。本指南旨在為程式設計初學者提供關於不同範數計算方法的清晰解釋，包括 1-範數、2-範數、無窮範數，以及 p-範數。以下是各種範數的定義及其計算方法。\n",
    "\n",
    "## 1. 1-範數 (L1 Norm)\n",
    "\n",
    "1-範數，也稱為「曼哈頓距離」，是向量中各元素絕對值的總和。\n",
    "\n",
    "- **計算公式：**\n",
    "\n",
    "  $$\n",
    "  \\|x\\|_1 = \\sum_{i=1}^{n} |x_i|\n",
    "  $$\n",
    "\n",
    "## 2. 2-範數 (L2 Norm)\n",
    "\n",
    "2-範數，亦即「歐幾里得範數」，是向量中各元素平方和的平方根。\n",
    "\n",
    "- **計算公式：**\n",
    "\n",
    "  $$\n",
    "  \\|x\\|_2 = \\sqrt{\\sum_{i=1}^{n} x_i^2}\n",
    "  $$\n",
    "\n",
    "## 3. 無窮範數 (L∞ Norm)\n",
    "\n",
    "無窮範數是向量中的最大絕對值。\n",
    "\n",
    "- **計算公式：**\n",
    "\n",
    "  $$\n",
    "  \\|x\\|_\\infty = \\max_i |x_i|\n",
    "  $$\n",
    "\n",
    "## 4. p-範數 (Lp Norm)\n",
    "\n",
    "p-範數是一個泛化的範數形式，適用於當 \\(p \\geq 1\\) 時。\n",
    "\n",
    "- **計算公式：**\n",
    "\n",
    "  $$\n",
    "  \\|x\\|_p = \\left(\\sum_{i=1}^{n} |x_i|^p\\right)^{\\frac{1}{p}}\n",
    "  $$\n",
    "\n",
    "範數在數學、物理學和工程學中扮演著關鍵角色。在機器學習領域，L1範數有助於促進模型的稀疏性，而L2範數常用於防止過擬合，也就是所謂的「L2正則化」。根據您的具體需求選擇合適的範數來進行計算。\n",
    "\n",
    "希望本指南能夠幫助您更好地理解和計算各種向量範數。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0717f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 2.]\n",
      " [4. 5.]]\n",
      "tensor([[6., 2.],\n",
      "        [4., 5.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 給定的 NumPy 陣列和 PyTorch 張量\n",
    "a_np = np.array([[6.0, 2.0],\n",
    "                 [4.0, 5.0]])\n",
    "a_torch = torch.tensor(a_np, dtype=torch.float)\n",
    "\n",
    "print(a_np)\n",
    "\n",
    "print(a_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f9d964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy L1範數: 10.0\n",
      "NumPy L2範數: 9.0\n",
      "PyTorch L1範數: tensor(17., dtype=torch.float64)\n",
      "PyTorch L2範數: tensor(9., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# NumPy 計算範數\n",
    "# 在 NumPy 的 np.linalg.norm 函數中，指定 ord=1 會計算矩陣的列和的最大值（即按列求絕對值之和的最大值）\n",
    "l1_norm_np = np.linalg.norm(a_np, ord=1)  # L1範數\n",
    "l2_norm_np = np.linalg.norm(a_np)  # L2範數，預設為ord=2\n",
    "\n",
    "# PyTorch 計算範數\n",
    "l1_norm_torch = torch.norm(a_torch, p=1)  # L1範數\n",
    "l2_norm_torch = torch.norm(a_torch)  # L2範數，預設為p=2\n",
    "\n",
    "print(\"NumPy L1範數:\", l1_norm_np)\n",
    "print(\"NumPy L2範數:\", l2_norm_np)\n",
    "print(\"PyTorch L1範數:\", l1_norm_torch)\n",
    "print(\"PyTorch L2範數:\", l2_norm_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "819109b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy L1範數（所有元素絕對值之和）: 17.0\n",
      "PyTorch L1範數（所有元素絕對值之和）: 17.0\n"
     ]
    }
   ],
   "source": [
    "# 使用 NumPy 計算所有元素的絕對值之和\n",
    "l1_norm_np = np.sum(np.abs(a_np))\n",
    "\n",
    "# 使用 PyTorch 計算所有元素的絕對值之和\n",
    "l1_norm_torch = torch.sum(torch.abs(a_torch))\n",
    "\n",
    "print(\"NumPy L1範數（所有元素絕對值之和）:\", l1_norm_np)\n",
    "print(\"PyTorch L1範數（所有元素絕對值之和）:\", l1_norm_torch.item())  # 使用 .item() 將單元素張量轉換為Python數值\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
